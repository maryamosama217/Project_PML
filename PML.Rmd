---
title: "PML"
author: "maryam"
date: "11/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical ML project
load requiered packages:
```{r}
library(caret)
library(lubridate)
library(e1071)
library(rattle)
```

first of all we download the data files:
```{r}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile ="C:/Users/DELL/Desktop/Project_PML/pml-training.csv" )
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile ="C:/Users/DELL/Desktop/Project_PML/pml-testing.csv" )

```
second we read the data files as CSV:
```{r}
training<-read.csv("C:/Users/DELL/Desktop/Project_PML/pml-training.csv")
testing<-read.csv("C:/Users/DELL/Desktop/Project_PML/pml-testing.csv")
```
recognizing the data
```{r}
summary(training)
```
some of the variables are wrongly identefied as characters
```{r}
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format =c("%d/%m/%y"))

for(i in colnames(training[,-c(1,2,5,6,160)])){training[,i]=as.numeric(training[,i])}

```

to look for missing values:
```{r}
table(sapply(training,function(x){sum(is.na(x))}))
```

note that some of the variables are measured once for a group pf observations and it return NA
for any other observation.
we need to split these two groups of variable and analyze them separately:
```{r}
groupvar1<-training[,sapply(training,Negate(anyNA)),drop=F]
groupvar1$classe=as.factor(groupvar1$classe);groupvar1$new_window=as.factor(groupvar1$new_window)

```

split training data into training,validation and testing:
```{r}
intrain<-createDataPartition(y=groupvar1$classe,p=0.6,list = F)
 train<-groupvar1[intrain,-c(1,2)]
vt<-groupvar1[-intrain,]
invalidate<-createDataPartition(y=vt$classe,list=F)
validate<-vt[invalidate,-c(1,2)]
test<-vt[-invalidate,-c(1,2)]
```
look at the correlation matrix:
```{r}
M<-cor(train[,-c(3,4,58)])
which(M>0.8&M!=1|M<(-0.8),arr.ind = T)
```
as we can see, many variables had correlation with each other so, we tend to use PCA:
```{r}
prepro<-preProcess(train[,-c(5,6,60)],method = "pca",thresh = 0.95)#====>27 PC needed
```
we will start building our model by trying several ML algorithms:
1)decision tree
```{r}
modelFit1<-train(classe~.,data=train[,-c(3,4)],method="rpart",na.action=na.exclude)
modelFit1$finalModel
fancyRpartPlot(modelFit1$finalModel)
modelFit1$results
pred<-predict(modelFit1,validate);validate$classepred<-pred==validate$classe
table(pred,validate$classe)
```
2)Bagging
```{r}
modelfit2<-train(classe~.,data=train[,-c(3,4)],method="treebag",na.action=na.exclude)
modelfit2$finalModel
fancyRpartPlot(modelFit1$finalModel)
modelfit2$results
pred2<-predict(modelfit2,validate);validate$classepred<-pred==validate$classe
table(pred2,validate$classe)
```
3)Boosting
```{r}
modelfit3<-train(classe~.,data=train[,-c(3,4)],method="gbm",na.action=na.exclude)
modelfit3$finalModel
modelfit3$results
pred3<-predict(modelfit3,validate);validate$classepred<-pred==validate$classe
table(pred3,validate$classe)
```
4)Random forest
```{r}
modelfit4<-train(classe~.,data=train[,-c(3,4)],method="rf",na.action=na.exclude)
modelfit4$finalModel
fancyRpartPlot(modelFit1$finalModel)
modelfit4$results
pred4<-predict(modelfit4,validate);validate$classepred<-pred==validate$classe
table(pred4,validate$classe)
```
as we can see,the Random forest model is the most accurate model on the validation set, so we select this model and apply in on the test sample:
```{r}
predtest<-predict(modelfit4,test);test$classepred<-pred==test$classe
table(predtest,test$classe)

```
now, we will use the same model to estimate the "testing" data
```{r}
predtesting<-predict(modelfit4,testing)
```

